{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural ODE for Collision-Coalescence Parameterization\n",
    "\n",
    "Plans:\n",
    "- ODEFunc: 4 → 50 (Tanh) → 50 (Tanh) → 50 (Tanh) → 4 (linear)\n",
    "- Solver: **Configurable** - RK4 (fixed-step, faster) or Dopri5 (adaptive, more accurate)\n",
    "- Training: Variable-length trajectories with masking also can continue training from saved checkpoints\n",
    "\n",
    "**Solver Comparison:**\n",
    "- **RK4**: Fixed-step (dt=20s), faster training\n",
    "- **Dopri5**: Adaptive step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdiffeq in /srv/conda/envs/notebook/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: torch>=1.5.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (2.9.1)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (1.16.3)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scipy>=1.4.0->torchdiffeq) (2.3.4)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.5)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.3)\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    'dt': 20.0,                  # Time step in seconds\n",
    "    'hidden_size': 50,           # Neurons per hidden layer\n",
    "    'n_layers': 3,               # Number of hidden layers\n",
    "    'batch_size': 4,             # Trajectories per batch\n",
    "    'learning_rate': 1e-4,       # Adam learning rate\n",
    "    'n_epochs': 100,             # Training epochs\n",
    "    'val_every': 3,              # Validation frequency\n",
    "    'max_grad_norm': 1.0,        # Gradient clipping threshold\n",
    "    'ode_solver': 'rk4',         # Choose: 'rk4' or 'dopri5'\n",
    "    'rtol': 1e-4,                # Relative tolerance (only for adaptive solvers like dopri5)\n",
    "    'atol': 1e-6,                # Absolute tolerance (only for adaptive solvers like dopri5)\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "data_dir = Path('/home/jovyan/cloud_microphysics/data/')\n",
    "train_path = data_dir / 'train_trajectories.pkl'\n",
    "val_path = data_dir / 'val_trajectories.pkl'\n",
    "stats_path = data_dir / 'moment_normalization_stats.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 575 training trajectories\n",
      "Loaded 144 validation trajectories\n",
      "\n",
      "Normalization stats loaded: ['asinh_scales', 'moment_scaler_mean', 'moment_scaler_std', 'moment_scaler']\n",
      "\n",
      "Sample trajectory keys: ['moments', 'length', 'ic_idx', 'moments_scaled']\n",
      "Moments shape: (174, 4)\n",
      "Trajectory length: 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed trajectories\n",
    "with open(train_path, 'rb') as f:\n",
    "    train_trajectories = pickle.load(f)\n",
    "\n",
    "with open(val_path, 'rb') as f:\n",
    "    val_trajectories = pickle.load(f)\n",
    "\n",
    "with open(stats_path, 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "print(f\"\\nLoaded {len(train_trajectories)} training trajectories\")\n",
    "print(f\"Loaded {len(val_trajectories)} validation trajectories\")\n",
    "print(f\"\\nNormalization stats loaded: {list(norm_stats.keys())}\")\n",
    "\n",
    "# Inspect sample trajectory\n",
    "sample_traj = train_trajectories[0]\n",
    "print(f\"\\nSample trajectory keys: {list(sample_traj.keys())}\")\n",
    "print(f\"Moments shape: {sample_traj['moments_scaled'].shape}\")\n",
    "print(f\"Trajectory length: {sample_traj['length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Training trajectories: 575\n",
      "Validation trajectories: 144\n",
      "\n",
      "Training trajectory lengths:\n",
      "  Min: 59, Max: 3599, Mean: 459.2\n",
      "Validation trajectory lengths:\n",
      "  Min: 59, Max: 2399, Mean: 448.5\n"
     ]
    }
   ],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataset for trajectory prediction with variable-length sequences.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - initial_state: (4,) tensor - Initial moment values\n",
    "            - trajectory: (length, 4) tensor - Full trajectory (variable length)\n",
    "            - length: int - Number of timesteps in this trajectory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trajectories, max_timesteps=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trajectories: List of trajectory dictionaries\n",
    "            max_timesteps: Optional maximum timesteps (for capping very long trajectories)\n",
    "        \"\"\"\n",
    "        self.trajectories = trajectories\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "\n",
    "        # Get scaled moments and actual length\n",
    "        moments_scaled = traj['moments_scaled']\n",
    "        length = traj['length']\n",
    "\n",
    "        # Optionally cap at max_timesteps\n",
    "        if self.max_timesteps:\n",
    "            length = min(length, self.max_timesteps)\n",
    "            moments_scaled = moments_scaled[:length]\n",
    "\n",
    "        # Initial state\n",
    "        initial_state = moments_scaled[0]  # (4,)\n",
    "\n",
    "        # Full trajectory\n",
    "        trajectory = moments_scaled  # (length, 4)\n",
    "\n",
    "        return {\n",
    "            'initial_state': torch.tensor(initial_state, dtype=torch.float32),\n",
    "            'trajectory': torch.tensor(trajectory, dtype=torch.float32),\n",
    "            'length': length\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_variable_length(batch):\n",
    "    \"\"\"Collate function to pad variable-length sequences and create masks.\n",
    "\n",
    "    Args:\n",
    "        batch: List of dictionaries from TrajectoryDataset\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - initial_states: (batch_size, 4) tensor\n",
    "            - trajectories: (batch_size, max_len, 4) tensor (padded)\n",
    "            - masks: (batch_size, max_len) bool tensor (True = valid, False = padded)\n",
    "            - lengths: (batch_size,) tensor of actual lengths\n",
    "    \"\"\"\n",
    "    lengths = [item['length'] for item in batch]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Initialize padded tensors\n",
    "    padded_trajectories = torch.zeros(batch_size, max_len, 4)\n",
    "    initial_states = torch.stack([item['initial_state'] for item in batch])\n",
    "\n",
    "    # Create mask (True = valid, False = padded)\n",
    "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
    "\n",
    "    # Fill in actual data and mask\n",
    "    for i, item in enumerate(batch):\n",
    "        length = item['length']\n",
    "        padded_trajectories[i, :length] = item['trajectory']\n",
    "        mask[i, :length] = True\n",
    "\n",
    "    return {\n",
    "        'initial_states': initial_states,\n",
    "        'trajectories': padded_trajectories,\n",
    "        'masks': mask,\n",
    "        'lengths': torch.tensor(lengths, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "\n",
    "def masked_mse_loss(predictions, targets, mask):\n",
    "    \"\"\"Compute MSE loss only on valid (non-padded) timesteps.\n",
    "\n",
    "    Args:\n",
    "        predictions: (batch_size, n_timesteps, 4) tensor\n",
    "        targets: (batch_size, n_timesteps, 4) tensor\n",
    "        mask: (batch_size, n_timesteps) bool tensor (True = valid)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Expand mask to match feature dimension\n",
    "    mask = mask.unsqueeze(-1)  # (batch_size, n_timesteps, 1)\n",
    "\n",
    "    # Compute squared errors\n",
    "    squared_errors = (predictions - targets) ** 2  # (batch_size, n_timesteps, 4)\n",
    "\n",
    "    # Apply mask and compute mean over valid elements only\n",
    "    masked_errors = squared_errors * mask\n",
    "    loss = masked_errors.sum() / mask.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Create datasets (no truncation - use all available timesteps)\n",
    "train_dataset = TrajectoryDataset(train_trajectories, max_timesteps=None)\n",
    "val_dataset = TrajectoryDataset(val_trajectories, max_timesteps=None)\n",
    "\n",
    "# Create dataloaders with custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_variable_length\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_variable_length\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Training trajectories: {len(train_dataset)}\")\n",
    "print(f\"Validation trajectories: {len(val_dataset)}\")\n",
    "\n",
    "# Show trajectory length distribution\n",
    "train_lengths = [traj['length'] for traj in train_trajectories]\n",
    "val_lengths = [traj['length'] for traj in val_trajectories]\n",
    "print(f\"\\nTraining trajectory lengths:\")\n",
    "print(f\"  Min: {min(train_lengths)}, Max: {max(train_lengths)}, Mean: {np.mean(train_lengths):.1f}\")\n",
    "print(f\"Validation trajectory lengths:\")\n",
    "print(f\"  Min: {min(val_lengths)}, Max: {max(val_lengths)}, Mean: {np.mean(val_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"Neural network that defines the derivative function dM/dt = f(M).\n",
    "    \n",
    "    Architecture: 4 → 50(Tanh) → 50(Tanh) → 50(Tanh) → 4(linear)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=50, n_layers=3):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer: 4 moments → hidden_size\n",
    "        layers.append(nn.Linear(4, hidden_size))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        # Hidden layers: hidden_size → hidden_size\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        # Output layer: hidden_size → 4 derivatives\n",
    "        layers.append(nn.Linear(hidden_size, 4))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: Scalar time (required by odeint, but not used for time-invariant dynamics)\n",
    "            y: State tensor of shape (batch_size, 4)\n",
    "        \n",
    "        Returns:\n",
    "            dy/dt: Derivative tensor of shape (batch_size, 4)\n",
    "        \"\"\"\n",
    "        return self.net(y)\n",
    "\n",
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    \"\"\"Wrapper class that integrates the ODE using torchdiffeq.\"\"\"\n",
    "\n",
    "    def __init__(self, ode_func, method='dopri5', rtol=1e-4, atol=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ode_func: ODEFunc instance\n",
    "            method: ODE solver method ('rk4' or 'dopri5')\n",
    "            rtol: Relative tolerance for adaptive solvers (ignored for rk4)\n",
    "            atol: Absolute tolerance for adaptive solvers (ignored for rk4)\n",
    "        \"\"\"\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.ode_func = ode_func\n",
    "        self.method = method\n",
    "        self.rtol = rtol\n",
    "        self.atol = atol\n",
    "\n",
    "    def forward(self, initial_state, t_span):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_state: Tensor of shape (batch_size, 4)\n",
    "            t_span: Tensor of time points to evaluate, shape (n_timesteps,)\n",
    "\n",
    "        Returns:\n",
    "            trajectory: Tensor of shape (n_timesteps, batch_size, 4)\n",
    "        \"\"\"\n",
    "        # For fixed-step solvers like rk4, rtol/atol are ignored\n",
    "        if self.method == 'rk4':\n",
    "            trajectory = odeint(\n",
    "                self.ode_func,\n",
    "                initial_state,\n",
    "                t_span,\n",
    "                method=self.method\n",
    "            )\n",
    "        else:\n",
    "            # For adaptive solvers like dopri5, use rtol/atol\n",
    "            trajectory = odeint(\n",
    "                self.ode_func,\n",
    "                initial_state,\n",
    "                t_span,\n",
    "                method=self.method,\n",
    "                rtol=self.rtol,\n",
    "                atol=self.atol\n",
    "            )\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 5,554 trainable parameters\n",
      "ODE Solver: rk4 (fixed-step, dt=20.0s)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=50, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=50, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "ode_func = ODEFunc(hidden_size=config['hidden_size'], n_layers=config['n_layers']).to(device)\n",
    "model = NeuralODE(\n",
    "    ode_func,\n",
    "    method=config['ode_solver'],\n",
    "    rtol=config['rtol'],\n",
    "    atol=config['atol']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model initialized with {n_params:,} trainable parameters\")\n",
    "\n",
    "# Print solver info\n",
    "if config['ode_solver'] == 'rk4':\n",
    "    print(f\"ODE Solver: {config['ode_solver']} (fixed-step, dt={config['dt']}s)\")\n",
    "else:\n",
    "    print(f\"ODE Solver: {config['ode_solver']} (adaptive, rtol={config['rtol']}, atol={config['atol']})\")\n",
    "\n",
    "print(model.ode_func.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'epochs': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from best_model.pt...\n",
      "Loaded training history with 3 epochs\n",
      "Resuming from Checkpoint\n",
      "  Epoch: 3\n",
      "  Train Loss: 4.511445\n",
      "  Val Loss: 4.648163\n",
      "  Config: {'dt': 20.0, 'hidden_size': 50, 'n_layers': 3, 'batch_size': 4, 'learning_rate': 0.0001, 'n_epochs': 100, 'val_every': 3, 'max_grad_norm': 1.0, 'ode_solver': 'rk4', 'rtol': 0.0001, 'atol': 1e-06}\n",
      "\n",
      "Will continue training from epoch 4 to 100\n"
     ]
    }
   ],
   "source": [
    "RESUME_TRAINING = True  # Set to True to continue from checkpoint\n",
    "CHECKPOINT_PATH = 'best_model.pt'  # Path to checkpoint file\n",
    "# The checkpoint contains model weights, optimizer state,\n",
    "# epoch number, and training history\n",
    "\n",
    "# Initialize training variables\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Load checkpoint if resuming\n",
    "if RESUME_TRAINING and Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Load model and optimizer states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load training progress\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
    "    \n",
    "    # Load history if available (for continuous loss curves)\n",
    "    if 'history' in checkpoint:\n",
    "        history = checkpoint['history']\n",
    "        print(f\"Loaded training history with {len(history['train_loss'])} epochs\")\n",
    "    \n",
    "    print(f\"Resuming from Checkpoint\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    if 'train_loss' in checkpoint:\n",
    "        print(f\"  Train Loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"  Val Loss: {checkpoint['val_loss']:.6f}\")\n",
    "    print(f\"  Config: {checkpoint['config']}\")\n",
    "    print(f\"\\nWill continue training from epoch {start_epoch + 1} to {config['n_epochs']}\")\n",
    "        \n",
    "elif RESUME_TRAINING and not Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"WARNING: RESUME_TRAINING=True but checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Starting fresh training for {config['n_epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, dt=20.0):\n",
    "    \"\"\"Train for one epoch with variable-length sequences.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_nan_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "        initial_states = batch['initial_states'].to(device)  # (batch_size, 4)\n",
    "        target_trajectories = batch['trajectories'].to(device)  # (batch_size, max_len, 4)\n",
    "        masks = batch['masks'].to(device)  # (batch_size, max_len)\n",
    "        \n",
    "        # Create dynamic t_span for this batch based on max length\n",
    "        max_len = target_trajectories.shape[1]\n",
    "        t_span = torch.arange(0, max_len, dtype=torch.float32, device=device) * dt\n",
    "        \n",
    "        # Forward pass: integrate ODE\n",
    "        pred_trajectory = model(initial_states, t_span)  # (max_len, batch_size, 4)\n",
    "        \n",
    "        # Reshape: (max_len, batch_size, 4) -> (batch_size, max_len, 4)\n",
    "        pred_trajectory = pred_trajectory.permute(1, 0, 2)\n",
    "        \n",
    "        # Check for NaN/Inf in predictions\n",
    "        if torch.isnan(pred_trajectory).any() or torch.isinf(pred_trajectory).any():\n",
    "            print(f\"\\nWARNING: NaN/Inf detected in predictions! Skipping batch.\")\n",
    "            n_nan_batches += 1\n",
    "            continue\n",
    "        \n",
    "        # Compute masked loss (only on valid timesteps)\n",
    "        loss = masked_mse_loss(pred_trajectory, target_trajectories, masks)\n",
    "        \n",
    "        # Check for NaN/Inf in loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"\\nWARNING: NaN/Inf loss detected! Skipping batch.\")\n",
    "            n_nan_batches += 1\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / max(len(train_loader) - n_nan_batches, 1)\n",
    "    if n_nan_batches > 0:\n",
    "        print(f\"\\nSkipped {n_nan_batches}/{len(train_loader)} batches due to NaN/Inf\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device, dt=20.0):\n",
    "    \"\"\"Validate the model with variable-length sequences.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_nan_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            initial_states = batch['initial_states'].to(device)\n",
    "            target_trajectories = batch['trajectories'].to(device)\n",
    "            masks = batch['masks'].to(device)\n",
    "            \n",
    "            # Create dynamic t_span for this batch\n",
    "            max_len = target_trajectories.shape[1]\n",
    "            t_span = torch.arange(0, max_len, dtype=torch.float32, device=device) * dt\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_trajectory = model(initial_states, t_span)\n",
    "            pred_trajectory = pred_trajectory.permute(1, 0, 2)\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(pred_trajectory).any() or torch.isinf(pred_trajectory).any():\n",
    "                n_nan_batches += 1\n",
    "                continue\n",
    "            \n",
    "            # Compute masked loss\n",
    "            loss = masked_mse_loss(pred_trajectory, target_trajectories, masks)\n",
    "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                n_nan_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / max(len(val_loader) - n_nan_batches, 1)\n",
    "    if n_nan_batches > 0:\n",
    "        print(f\"  Validation: Skipped {n_nan_batches}/{len(val_loader)} batches due to NaN/Inf\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training from epoch 4...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Loss: 4.920209, Val Loss: 4.251313\n",
      "  → Saved best model (val_loss: 4.251313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train Loss: 4.541486, Val Loss: 3.672335\n",
      "  → Saved best model (val_loss: 3.672335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train Loss: 4.600614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train Loss: 4.794000, Val Loss: 7.030597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 9/143 [00:10<02:20,  1.05s/it]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"\\nStarting training from epoch {start_epoch + 1}...\\n\")\n",
    "\n",
    "for epoch in range(start_epoch, config['n_epochs']):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, dt=config['dt'])\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    if (epoch + 1) % config['val_every'] == 0:\n",
    "        val_loss = validate(model, val_loader, device, dt=config['dt'])\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config['n_epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_loss': train_loss,\n",
    "                'config': config,\n",
    "                'history': history  # Save history for continuous loss curves\n",
    "            }, 'best_model.pt')\n",
    "            print(f\"  → Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    else:\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config['n_epochs']} - Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"Trained for {len(history['train_loss'])} total epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Train loss\n",
    "ax.plot(range(1, len(history['train_loss']) + 1), history['train_loss'], \n",
    "        label='Train Loss', alpha=0.7)\n",
    "\n",
    "# Val loss\n",
    "ax.plot(history['epochs'], history['val_loss'], \n",
    "        label='Val Loss', marker='o', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "print(f\"Best val loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model checkpoint\n",
    "checkpoint = torch.load('best_model.pt', map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} (val_loss: {checkpoint['val_loss']:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select validation samples to visualize\n",
    "n_samples = 10\n",
    "sample_indices = np.random.choice(len(val_dataset), n_samples, replace=True)\n",
    "\n",
    "# Moment names\n",
    "moment_names = ['qc (cloud water)', 'nc (cloud droplets)', 'qr (rain water)', 'nr (rain drops)']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_samples, 4, figsize=(16, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        # Get data\n",
    "        sample = val_dataset[sample_idx]\n",
    "        initial_state = sample['initial_state'].unsqueeze(0).to(device)  # (1, 4)\n",
    "        target_trajectory = sample['trajectory'].cpu().numpy()  # (length, 4)\n",
    "        length = sample['length']\n",
    "        \n",
    "        # Create t_span for this specific trajectory\n",
    "        t_span_sample = torch.arange(0, length, dtype=torch.float32, device=device) * config['dt']\n",
    "        \n",
    "        # Predict\n",
    "        pred_trajectory = model(initial_state, t_span_sample)  # (length, 1, 4)\n",
    "        pred_trajectory = pred_trajectory.squeeze(1).cpu().numpy()  # (length, 4)\n",
    "        \n",
    "        # Time axis\n",
    "        time_axis = t_span_sample.cpu().numpy()\n",
    "        \n",
    "        # Plot each moment\n",
    "        for j in range(4):\n",
    "            ax = axes[i, j]\n",
    "            ax.plot(time_axis, target_trajectory[:, j], 'k-', label='True', linewidth=2)\n",
    "            ax.plot(time_axis, pred_trajectory[:, j], 'r--', label='Predicted', linewidth=2)\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title(moment_names[j], fontsize=12)\n",
    "            if i == n_samples - 1:\n",
    "                ax.set_xlabel('Time (s)', fontsize=10)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'Sample {i+1}\\n(len={length})\\nNormalized Value', fontsize=10)\n",
    "            \n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics on full validation set with variable-length sequences\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_masks = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        initial_states = batch['initial_states'].to(device)\n",
    "        target_trajectories = batch['trajectories']  # Keep on CPU for now\n",
    "        masks = batch['masks']  # (batch_size, max_len)\n",
    "        \n",
    "        # Create dynamic t_span for this batch\n",
    "        max_len = target_trajectories.shape[1]\n",
    "        t_span = torch.arange(0, max_len, dtype=torch.float32, device=device) * config['dt']\n",
    "        \n",
    "        # Predict\n",
    "        pred_trajectory = model(initial_states, t_span)  # (max_len, batch_size, 4)\n",
    "        pred_trajectory = pred_trajectory.permute(1, 0, 2).cpu()  # (batch_size, max_len, 4)\n",
    "        \n",
    "        all_predictions.append(pred_trajectory.numpy())\n",
    "        all_targets.append(target_trajectories.numpy())\n",
    "        all_masks.append(masks.numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)  # (n_samples, max_len, 4)\n",
    "all_targets = np.concatenate(all_targets, axis=0)  # (n_samples, max_len, 4)\n",
    "all_masks = np.concatenate(all_masks, axis=0)  # (n_samples, max_len)\n",
    "\n",
    "# Compute metrics per moment (only on valid timesteps)\n",
    "print(\"\\nValidation Metrics (normalized space, masked):\")\n",
    "\n",
    "for i, name in enumerate(moment_names):\n",
    "    # Extract valid predictions and targets for this moment\n",
    "    valid_mask = all_masks  # (n_samples, max_len)\n",
    "    \n",
    "    # MSE (only on valid timesteps)\n",
    "    squared_errors = (all_predictions[:, :, i] - all_targets[:, :, i]) ** 2\n",
    "    mse = (squared_errors * valid_mask).sum() / valid_mask.sum()\n",
    "    \n",
    "    # MAE (only on valid timesteps)\n",
    "    abs_errors = np.abs(all_predictions[:, :, i] - all_targets[:, :, i])\n",
    "    mae = (abs_errors * valid_mask).sum() / valid_mask.sum()\n",
    "    \n",
    "    # R² score (only on valid timesteps)\n",
    "    valid_targets = all_targets[:, :, i][valid_mask]\n",
    "    valid_predictions = all_predictions[:, :, i][valid_mask]\n",
    "    ss_res = np.sum((valid_targets - valid_predictions) ** 2)\n",
    "    ss_tot = np.sum((valid_targets - np.mean(valid_targets)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    print(f\"{name:25s} - MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "\n",
    "# Overall metrics\n",
    "overall_squared_errors = (all_predictions - all_targets) ** 2\n",
    "all_masks_expanded = all_masks[:, :, np.newaxis]  # (n_samples, max_len, 1)\n",
    "overall_mse = (overall_squared_errors * all_masks_expanded).sum() / all_masks_expanded.sum()\n",
    "\n",
    "overall_abs_errors = np.abs(all_predictions - all_targets)\n",
    "overall_mae = (overall_abs_errors * all_masks_expanded).sum() / all_masks_expanded.sum()\n",
    "\n",
    "print(f\"{'Overall':25s} - MSE: {overall_mse:.6f}, MAE: {overall_mae:.6f}\")\n",
    "\n",
    "# Report total number of timesteps evaluated\n",
    "total_valid_timesteps = all_masks.sum()\n",
    "total_possible_timesteps = all_masks.size\n",
    "print(f\"\\nEvaluated {int(total_valid_timesteps):,} valid timesteps out of {total_possible_timesteps:,} total ({100*total_valid_timesteps/total_possible_timesteps:.1f}% non-padded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MAE as a function of time (accounting for variable-length sequences)\n",
    "# We'll compute MAE at each time index, but only average over trajectories that have data at that index\n",
    "\n",
    "max_len = all_predictions.shape[1]\n",
    "mae_over_time = np.zeros((max_len, 4))\n",
    "count_over_time = np.zeros(max_len)\n",
    "\n",
    "for t in range(max_len):\n",
    "    # Find which samples have valid data at time t\n",
    "    valid_at_t = all_masks[:, t]  # (n_samples,)\n",
    "    count_over_time[t] = valid_at_t.sum()\n",
    "    \n",
    "    if count_over_time[t] > 0:\n",
    "        # Compute MAE for each moment at time t\n",
    "        for i in range(4):\n",
    "            errors = np.abs(all_predictions[:, t, i] - all_targets[:, t, i])\n",
    "            mae_over_time[t, i] = (errors * valid_at_t).sum() / count_over_time[t]\n",
    "\n",
    "# Create time axis in seconds\n",
    "time_axis = np.arange(max_len) * config['dt']\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot MAE over time\n",
    "for i, name in enumerate(moment_names):\n",
    "    ax1.plot(time_axis, mae_over_time[:, i], label=name, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Time (s)', fontsize=12)\n",
    "ax1.set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "ax1.set_title('Prediction Error Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot number of trajectories available at each time\n",
    "ax2.plot(time_axis, count_over_time, 'k-', linewidth=2)\n",
    "ax2.fill_between(time_axis, 0, count_over_time, alpha=0.3)\n",
    "ax2.set_xlabel('Time (s)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Trajectories', fontsize=12)\n",
    "ax2.set_title('Data Availability Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTime range: 0 to {time_axis[count_over_time > 0][-1]:.0f} seconds\")\n",
    "print(f\"At least one trajectory extends to: {time_axis[-1]:.0f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
