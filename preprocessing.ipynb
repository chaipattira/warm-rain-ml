{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3752ecf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:02.363496Z",
     "iopub.status.busy": "2025-10-31T19:36:02.363340Z",
     "iopub.status.idle": "2025-10-31T19:36:03.612637Z",
     "shell.execute_reply": "2025-10-31T19:36:03.612069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdiffeq in /srv/conda/envs/notebook/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: torch>=1.5.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (2.5.1.post303)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scipy>=1.4.0->torchdiffeq) (2.2.6)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (4.14.1)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.5)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46d87c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:03.614813Z",
     "iopub.status.busy": "2025-10-31T19:36:03.614602Z",
     "iopub.status.idle": "2025-10-31T19:36:06.035279Z",
     "shell.execute_reply": "2025-10-31T19:36:06.034795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchdiffeq import odeint_adjoint\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf58ffa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:06.037132Z",
     "iopub.status.busy": "2025-10-31T19:36:06.036840Z",
     "iopub.status.idle": "2025-10-31T19:36:16.369171Z",
     "shell.execute_reply": "2025-10-31T19:36:16.368676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked data - Min: 0.00e+00, Max: 6.54e+08\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = Path(\"./data\")\n",
    "test_data = np.load(data_dir / \"test_arr.npz\")\n",
    "data_array = test_data['data']  # shape (time, features, ensemble, ics)\n",
    "mask_array = test_data['mask']  # same shape as data_array\n",
    "\n",
    "# Apply masking\n",
    "data_masked = np.ma.MaskedArray(data_array, mask=mask_array)\n",
    "print(f\"\\nMasked data - Min: {data_masked.min():.2e}, Max: {data_masked.max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6fd4235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:16.371150Z",
     "iopub.status.busy": "2025-10-31T19:36:16.370761Z",
     "iopub.status.idle": "2025-10-31T19:36:16.373941Z",
     "shell.execute_reply": "2025-10-31T19:36:16.373541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ICs: 100\n",
      "Train ICs: 90, Val ICs: 10\n"
     ]
    }
   ],
   "source": [
    "# Use 90% of the data for training\n",
    "n_ics_total = data_array.shape[2]\n",
    "n_ics_train = int(0.9 * n_ics_total)\n",
    "print(f\"Total ICs: {n_ics_total}\")\n",
    "\n",
    "# Split: first 90% for train, rest for validation\n",
    "train_indices = np.arange(n_ics_train)\n",
    "val_indices = np.arange(n_ics_train, n_ics_total)\n",
    "print(f\"Train ICs: {len(train_indices)}, Val ICs: {len(val_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7df0aef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:16.375494Z",
     "iopub.status.busy": "2025-10-31T19:36:16.375331Z",
     "iopub.status.idle": "2025-10-31T19:36:19.553008Z",
     "shell.execute_reply": "2025-10-31T19:36:19.552532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moment indices: [1, 2, 3, 4]\n",
      "\n",
      "Ensemble mean shape: (3599, 18, 100)\n"
     ]
    }
   ],
   "source": [
    "# Extract features: moments [1-4]\n",
    "moment_indices = [1, 2, 3, 4]  # qc, nc, qr, nr\n",
    "\n",
    "print(f\"Moment indices: {moment_indices}\")\n",
    "\n",
    "# Compute ensemble mean (average over the 100 instances dimension)\n",
    "data_mean = data_masked.mean(axis=3)  # Shape: (time, features, ics)\n",
    "print(f\"\\nEnsemble mean shape: {data_mean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90b6f7c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:19.554650Z",
     "iopub.status.busy": "2025-10-31T19:36:19.554477Z",
     "iopub.status.idle": "2025-10-31T19:36:19.585995Z",
     "shell.execute_reply": "2025-10-31T19:36:19.585561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training trajectories: 90\n",
      "Number of validation trajectories: 10\n",
      "\n",
      "Example trajectory (train):\n",
      "  Trajectory 0: 130 timesteps\n",
      "  Trajectory 1: 779 timesteps\n",
      "  Trajectory 2: 324 timesteps\n",
      "  Trajectory 3: 149 timesteps\n",
      "  Trajectory 4: 59 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Create trajectories with proper masking\n",
    "def extract_trajectories(data_mean, ic_indices, moment_indices):\n",
    "    \"\"\"\n",
    "    Extract valid trajectories for given IC indices.\n",
    "    Returns list of dicts with 'moments', 'length', 'ic_idx'\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    \n",
    "    for ic_idx in ic_indices:\n",
    "        # Extract moments for this IC\n",
    "        moments = data_mean[:, moment_indices, ic_idx]  # (time, 4)\n",
    "        \n",
    "        # Find valid timesteps (check first moment)\n",
    "        valid_mask = ~moments[:, 0].mask\n",
    "        n_valid = valid_mask.sum()\n",
    "        \n",
    "        if n_valid > 1:  # Need at least 2 timesteps for derivatives\n",
    "            # Extract only valid data\n",
    "            moments_valid = moments[valid_mask].data  # (n_valid, 4)\n",
    "            \n",
    "            trajectories.append({\n",
    "                'moments': moments_valid,\n",
    "                'length': n_valid,\n",
    "                'ic_idx': ic_idx\n",
    "            })\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "train_trajectories = extract_trajectories(data_mean, train_indices, moment_indices)\n",
    "val_trajectories = extract_trajectories(data_mean, val_indices, moment_indices)\n",
    "\n",
    "print(f\"\\nNumber of training trajectories: {len(train_trajectories)}\")\n",
    "print(f\"Number of validation trajectories: {len(val_trajectories)}\")\n",
    "print(f\"\\nExample trajectory (train):\")\n",
    "for i in range(min(5, len(train_trajectories))):\n",
    "    print(f\"  Trajectory {i}: {train_trajectories[i]['length']} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wd48jqw6ew",
   "metadata": {},
   "source": [
    "Moments span many orders of magnitude (7-9) with heavy skew and contain zeros, so I use asinh transformation + z-score:\n",
    "1. Compute optimal scales for each moment (median-based)\n",
    "2. Apply asinh(x / scale) transformation\n",
    "3. Apply standard scaling to normalized asinh values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ivndw8uri7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training timesteps: 42843\n",
      "\n",
      "Raw moment statistics:\n",
      "  qc - Min: 3.50e-11, Max: 2.00e-03, Median: 8.42e-05\n",
      "  nc - Min: 6.52e+01, Max: 6.54e+08, Median: 1.33e+07\n",
      "  qr - Min: 1.74e-23, Max: 5.59e-14, Median: 9.57e-16\n",
      "  nr - Min: 0.00e+00, Max: 2.00e-03, Median: 2.00e-04\n",
      "\n",
      "Asinh scales (using median):\n",
      "  qc: 8.42e-05\n",
      "  nc: 1.33e+07\n",
      "  qr: 9.57e-16\n",
      "  nr: 2.00e-04\n"
     ]
    }
   ],
   "source": [
    "all_moments_train = []\n",
    "for traj in train_trajectories:\n",
    "    all_moments_train.append(traj['moments'])\n",
    "\n",
    "all_moments_train = np.vstack(all_moments_train)  # Shape: (total_timesteps, 4)\n",
    "print(f\"Total training timesteps: {all_moments_train.shape[0]}\")\n",
    "print(f\"\\nRaw moment statistics:\")\n",
    "print(f\"  qc - Min: {all_moments_train[:, 0].min():.2e}, Max: {all_moments_train[:, 0].max():.2e}, Median: {np.median(all_moments_train[:, 0]):.2e}\")\n",
    "print(f\"  nc - Min: {all_moments_train[:, 1].min():.2e}, Max: {all_moments_train[:, 1].max():.2e}, Median: {np.median(all_moments_train[:, 1]):.2e}\")\n",
    "print(f\"  qr - Min: {all_moments_train[:, 2].min():.2e}, Max: {all_moments_train[:, 2].max():.2e}, Median: {np.median(all_moments_train[:, 2]):.2e}\")\n",
    "print(f\"  nr - Min: {all_moments_train[:, 3].min():.2e}, Max: {all_moments_train[:, 3].max():.2e}, Median: {np.median(all_moments_train[:, 3]):.2e}\")\n",
    "\n",
    "# Compute optimal asinh scales (use median as characteristic scale)\n",
    "# asinh(median) â‰ˆ 1\n",
    "asinh_scales = np.array([\n",
    "    np.median(all_moments_train[:, 0]),  # qc\n",
    "    np.median(all_moments_train[:, 1]),  # nc\n",
    "    np.median(all_moments_train[:, 2]),  # qr\n",
    "    np.median(all_moments_train[:, 3])   # nr\n",
    "])\n",
    "\n",
    "print(f\"\\nAsinh scales (using median):\")\n",
    "print(f\"  qc: {asinh_scales[0]:.2e}\")\n",
    "print(f\"  nc: {asinh_scales[1]:.2e}\")\n",
    "print(f\"  qr: {asinh_scales[2]:.2e}\")\n",
    "print(f\"  nr: {asinh_scales[3]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "akspq2p1u4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moment scaler parameters:\n",
      "  Means: [1.21560717 1.27897289 1.11573218 0.97506293]\n",
      "  Stds: [1.23480528 1.32050034 1.17365607 0.90159444]\n"
     ]
    }
   ],
   "source": [
    "# Apply asinh transformation to training data\n",
    "moments_asinh_train = np.arcsinh(all_moments_train / asinh_scales)\n",
    "\n",
    "# Fit StandardScaler on asinh-transformed moments\n",
    "moment_scaler = StandardScaler()\n",
    "moment_scaler.fit(moments_asinh_train)\n",
    "\n",
    "print(f\"\\nMoment scaler parameters:\")\n",
    "print(f\"  Means: {moment_scaler.mean_}\")\n",
    "print(f\"  Stds: {moment_scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "gb2qi4871m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply full transformation (asinh + z-score) to all trajectories\n",
    "def transform_moments(moments, asinh_scales, moment_scaler):\n",
    "    \"\"\"Apply asinh transformation followed by z-score normalization.\"\"\"\n",
    "    moments_asinh = np.arcsinh(moments / asinh_scales)\n",
    "    moments_normalized = moment_scaler.transform(moments_asinh)\n",
    "    return moments_normalized\n",
    "\n",
    "# Apply to training trajectories\n",
    "for traj in train_trajectories:\n",
    "    traj['moments_scaled'] = transform_moments(traj['moments'], asinh_scales, moment_scaler)\n",
    "\n",
    "# Apply to validation trajectories\n",
    "for traj in val_trajectories:\n",
    "    traj['moments_scaled'] = transform_moments(traj['moments'], asinh_scales, moment_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4a31a",
   "metadata": {},
   "source": [
    "We now incorporate Transformed moments to all trajectories as 'moments_scaled' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "uu8ps5kko3p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asinh scales: [8.42491240e-05 1.33119082e+07 9.57075087e-16 1.99577122e-04]\n",
      "Scaler means: [1.21560717 1.27897289 1.11573218 0.97506293]\n",
      "Scaler stds: [1.23480528 1.32050034 1.17365607 0.90159444]\n"
     ]
    }
   ],
   "source": [
    "# Save normalization parameters for inference\n",
    "moment_normalization_stats = {\n",
    "    'asinh_scales': asinh_scales,\n",
    "    'moment_scaler_mean': moment_scaler.mean_,\n",
    "    'moment_scaler_std': moment_scaler.scale_,\n",
    "    'moment_scaler': moment_scaler\n",
    "}\n",
    "\n",
    "with open(data_dir / 'moment_normalization_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(moment_normalization_stats, f)\n",
    "\n",
    "print(f\"Asinh scales: {asinh_scales}\")\n",
    "print(f\"Scaler means: {moment_scaler.mean_}\")\n",
    "print(f\"Scaler stds: {moment_scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34fe7d",
   "metadata": {},
   "source": [
    "Each trajectory contains:\n",
    "- `moments`: raw moment data (n_timesteps, 4)\n",
    "- `moments_scaled`: asinh + z-score normalized moments (n_timesteps, 4)\n",
    "- `length`: number of timesteps\n",
    "- `ic_idx`: initial condition index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "w4nxpz5qpkm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled moment statistics:\n",
      "  qc - Mean: +0.000000, Std: 1.000000, Range: [-0.98, +2.14]\n",
      "  nc - Mean: -0.000000, Std: 1.000000, Range: [-0.97, +2.51]\n",
      "  qr - Mean: -0.000000, Std: 1.000000, Range: [-0.95, +3.10]\n",
      "  nr - Mean: -0.000000, Std: 1.000000, Range: [-1.08, +2.24]\n",
      "No NaN or Inf values detected\n"
     ]
    }
   ],
   "source": [
    "# Collect all scaled moments from training data\n",
    "all_moments_scaled_train = np.vstack([traj['moments_scaled'] for traj in train_trajectories])\n",
    "\n",
    "print(f\"\\nScaled moment statistics:\")\n",
    "for i, name in enumerate(['qc', 'nc', 'qr', 'nr']):\n",
    "    mean = all_moments_scaled_train[:, i].mean()\n",
    "    std = all_moments_scaled_train[:, i].std()\n",
    "    min_val = all_moments_scaled_train[:, i].min()\n",
    "    max_val = all_moments_scaled_train[:, i].max()\n",
    "    print(f\"  {name} - Mean: {mean:+.6f}, Std: {std:.6f}, Range: [{min_val:+.2f}, {max_val:+.2f}]\")\n",
    "\n",
    "# Check for NaN or Inf values\n",
    "if np.any(np.isnan(all_moments_scaled_train)) or np.any(np.isinf(all_moments_scaled_train)):\n",
    "    print(\"WARNING: NaN or Inf values detected in scaled moments!\")\n",
    "else:\n",
    "    print(\"No NaN or Inf values detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed4919aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:19.728842Z",
     "iopub.status.busy": "2025-10-31T19:36:19.728488Z",
     "iopub.status.idle": "2025-10-31T19:36:19.759324Z",
     "shell.execute_reply": "2025-10-31T19:36:19.758859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save processed trajectories\n",
    "with open(data_dir / \"train_trajectories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_trajectories, f)\n",
    "\n",
    "with open(data_dir / \"val_trajectories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_trajectories, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
