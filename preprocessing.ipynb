{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3752ecf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:02.363496Z",
     "iopub.status.busy": "2025-10-31T19:36:02.363340Z",
     "iopub.status.idle": "2025-10-31T19:36:03.612637Z",
     "shell.execute_reply": "2025-10-31T19:36:03.612069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdiffeq in /srv/conda/envs/notebook/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: torch>=1.5.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (2.5.1.post303)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torchdiffeq) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scipy>=1.4.0->torchdiffeq) (2.2.6)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (4.14.1)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.5)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.5.0->torchdiffeq) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46d87c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:03.614813Z",
     "iopub.status.busy": "2025-10-31T19:36:03.614602Z",
     "iopub.status.idle": "2025-10-31T19:36:06.035279Z",
     "shell.execute_reply": "2025-10-31T19:36:06.034795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchdiffeq import odeint_adjoint\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf58ffa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:06.037132Z",
     "iopub.status.busy": "2025-10-31T19:36:06.036840Z",
     "iopub.status.idle": "2025-10-31T19:36:16.369171Z",
     "shell.execute_reply": "2025-10-31T19:36:16.368676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked data - Min: 0.00e+00, Max: 6.54e+08\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = Path(\"./data\")\n",
    "test_data = np.load(data_dir / \"test_arr.npz\")\n",
    "data_array = test_data['data']  # shape (time, features, ensemble, ics)\n",
    "mask_array = test_data['mask']  # same shape as data_array\n",
    "\n",
    "# Apply masking\n",
    "data_masked = np.ma.MaskedArray(data_array, mask=mask_array)\n",
    "print(f\"\\nMasked data - Min: {data_masked.min():.2e}, Max: {data_masked.max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6fd4235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:16.371150Z",
     "iopub.status.busy": "2025-10-31T19:36:16.370761Z",
     "iopub.status.idle": "2025-10-31T19:36:16.373941Z",
     "shell.execute_reply": "2025-10-31T19:36:16.373541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ICs: 100\n",
      "Train ICs: 80, Val ICs: 20\n"
     ]
    }
   ],
   "source": [
    "# Use 80% of the data for training\n",
    "n_ics_total = data_array.shape[2]\n",
    "n_ics_train = int(0.8 * n_ics_total)\n",
    "print(f\"Total ICs: {n_ics_total}\")\n",
    "\n",
    "# Split: first 80% for train, rest for validation\n",
    "train_indices = np.arange(n_ics_train)\n",
    "val_indices = np.arange(n_ics_train, n_ics_total)\n",
    "print(f\"Train ICs: {len(train_indices)}, Val ICs: {len(val_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df0aef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:16.375494Z",
     "iopub.status.busy": "2025-10-31T19:36:16.375331Z",
     "iopub.status.idle": "2025-10-31T19:36:19.553008Z",
     "shell.execute_reply": "2025-10-31T19:36:19.552532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moment indices: [1, 2, 3, 4]\n",
      "Environmental parameter indices: [14, 15, 16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble mean shape: (3599, 18, 100)\n"
     ]
    }
   ],
   "source": [
    "# Extract features: moments [1-4] and environmental params [14-16]\n",
    "moment_indices = [1, 2, 3, 4]  # qc, nc, qr, nr\n",
    "env_indices = [14, 15, 16]  # q_w0, r_0, ν\n",
    "\n",
    "print(f\"Moment indices: {moment_indices}\")\n",
    "print(f\"Environmental parameter indices: {env_indices}\")\n",
    "\n",
    "# Compute ensemble mean (average over the 100 instances dimension)\n",
    "data_mean = data_masked.mean(axis=3)  # Shape: (time, features, ics)\n",
    "print(f\"\\nEnsemble mean shape: {data_mean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b6f7c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:19.554650Z",
     "iopub.status.busy": "2025-10-31T19:36:19.554477Z",
     "iopub.status.idle": "2025-10-31T19:36:19.585995Z",
     "shell.execute_reply": "2025-10-31T19:36:19.585561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training trajectories: 80\n",
      "Number of validation trajectories: 20\n",
      "\n",
      "Example trajectory lengths (train):\n",
      "  Trajectory 0: 130 timesteps\n",
      "  Trajectory 1: 779 timesteps\n",
      "  Trajectory 2: 324 timesteps\n",
      "  Trajectory 3: 149 timesteps\n",
      "  Trajectory 4: 59 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Create trajectories with proper masking\n",
    "def extract_trajectories(data_mean, ic_indices, moment_indices, env_indices):\n",
    "    \"\"\"\n",
    "    Extract valid trajectories for given IC indices.\n",
    "    Returns list of dicts with 'moments', 'env_params', 'length'\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    \n",
    "    for ic_idx in ic_indices:\n",
    "        # Extract moments and env params for this IC\n",
    "        moments = data_mean[:, moment_indices, ic_idx]  # (time, 4)\n",
    "        env_params = data_mean[0, env_indices, ic_idx]  # (3,) - constant across time\n",
    "        \n",
    "        # Find valid timesteps (check first moment)\n",
    "        valid_mask = ~moments[:, 0].mask\n",
    "        n_valid = valid_mask.sum()\n",
    "        \n",
    "        if n_valid > 1:  # Need at least 2 timesteps for derivatives\n",
    "            # Extract only valid data\n",
    "            moments_valid = moments[valid_mask].data  # (n_valid, 4)\n",
    "            env_params_valid = env_params.data  # (3,)\n",
    "            \n",
    "            trajectories.append({\n",
    "                'moments': moments_valid,\n",
    "                'env_params': env_params_valid,\n",
    "                'length': n_valid,\n",
    "                'ic_idx': ic_idx\n",
    "            })\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "train_trajectories = extract_trajectories(data_mean, train_indices, moment_indices, env_indices)\n",
    "val_trajectories = extract_trajectories(data_mean, val_indices, moment_indices, env_indices)\n",
    "\n",
    "print(f\"\\nNumber of training trajectories: {len(train_trajectories)}\")\n",
    "print(f\"Number of validation trajectories: {len(val_trajectories)}\")\n",
    "print(f\"\\nExample trajectory lengths (train):\")\n",
    "for i in range(min(5, len(train_trajectories))):\n",
    "    print(f\"  Trajectory {i}: {train_trajectories[i]['length']} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cfb4c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:19.712201Z",
     "iopub.status.busy": "2025-10-31T19:36:19.712025Z",
     "iopub.status.idle": "2025-10-31T19:36:19.727162Z",
     "shell.execute_reply": "2025-10-31T19:36:19.726752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environmental parameter statistics (before scaling):\n",
      "  Mean: [8.84999941e-04 1.19000000e-05 2.10625000e+00]\n",
      "  Std: [4.63707871e-04 2.02854569e-06 1.19030918e+00]\n",
      "\n",
      "Environmental parameter statistics (after scaling):\n",
      "  Mean: [ 8.32667268e-16  4.87387908e-15 -1.74686654e-16]\n",
      "  Std: [1. 1. 1.]\n",
      "\n",
      "Scaler saved to data/env_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Fit StandardScaler on environmental parameters (training data only)\n",
    "env_params_train = np.array([traj['env_params'] for traj in train_trajectories])\n",
    "env_scaler = StandardScaler()\n",
    "env_scaler.fit(env_params_train)\n",
    "\n",
    "print(f\"\\nEnvironmental parameter statistics (before scaling):\")\n",
    "print(f\"  Mean: {env_params_train.mean(axis=0)}\")\n",
    "print(f\"  Std: {env_params_train.std(axis=0)}\")\n",
    "\n",
    "# Apply scaling to all trajectories\n",
    "for traj in train_trajectories:\n",
    "    traj['env_params_scaled'] = env_scaler.transform(traj['env_params'].reshape(1, -1)).flatten()\n",
    "\n",
    "for traj in val_trajectories:\n",
    "    traj['env_params_scaled'] = env_scaler.transform(traj['env_params'].reshape(1, -1)).flatten()\n",
    "\n",
    "print(f\"\\nEnvironmental parameter statistics (after scaling):\")\n",
    "env_params_scaled_train = np.array([traj['env_params_scaled'] for traj in train_trajectories])\n",
    "print(f\"  Mean: {env_params_scaled_train.mean(axis=0)}\")\n",
    "print(f\"  Std: {env_params_scaled_train.std(axis=0)}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "with open(data_dir / 'env_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(env_scaler, f)\n",
    "print(\"\\nScaler saved to data/env_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wd48jqw6ew",
   "metadata": {},
   "source": [
    "Moments span many orders of magnitude and have heavy-tailed distributions, so we use asinh transformation:\n",
    "1. Compute optimal scales for each moment variable\n",
    "2. Apply asinh(x / scale) transformation\n",
    "3. Apply standard scaling to normalized asinh values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ivndw8uri7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training timesteps: 39371\n",
      "\n",
      "Raw moment statistics:\n",
      "  qc - Min: 3.50e-11, Max: 2.00e-03, Median: 7.82e-05\n",
      "  nc - Min: 6.52e+01, Max: 6.54e+08, Median: 1.27e+07\n",
      "  qr - Min: 1.74e-23, Max: 5.59e-14, Median: 8.67e-16\n",
      "  nr - Min: 0.00e+00, Max: 2.00e-03, Median: 2.00e-04\n",
      "\n",
      "Asinh scales (using median):\n",
      "  qc: 7.82e-05\n",
      "  nc: 1.27e+07\n",
      "  qr: 8.67e-16\n",
      "  nr: 2.00e-04\n"
     ]
    }
   ],
   "source": [
    "all_moments_train = []\n",
    "for traj in train_trajectories:\n",
    "    all_moments_train.append(traj['moments'])\n",
    "\n",
    "all_moments_train = np.vstack(all_moments_train)  # Shape: (total_timesteps, 4)\n",
    "print(f\"Total training timesteps: {all_moments_train.shape[0]}\")\n",
    "print(f\"\\nRaw moment statistics:\")\n",
    "print(f\"  qc - Min: {all_moments_train[:, 0].min():.2e}, Max: {all_moments_train[:, 0].max():.2e}, Median: {np.median(all_moments_train[:, 0]):.2e}\")\n",
    "print(f\"  nc - Min: {all_moments_train[:, 1].min():.2e}, Max: {all_moments_train[:, 1].max():.2e}, Median: {np.median(all_moments_train[:, 1]):.2e}\")\n",
    "print(f\"  qr - Min: {all_moments_train[:, 2].min():.2e}, Max: {all_moments_train[:, 2].max():.2e}, Median: {np.median(all_moments_train[:, 2]):.2e}\")\n",
    "print(f\"  nr - Min: {all_moments_train[:, 3].min():.2e}, Max: {all_moments_train[:, 3].max():.2e}, Median: {np.median(all_moments_train[:, 3]):.2e}\")\n",
    "\n",
    "# Compute optimal asinh scales (use median as characteristic scale)\n",
    "# asinh(median) ≈ 1\n",
    "asinh_scales = np.array([\n",
    "    np.median(all_moments_train[:, 0]),  # qc\n",
    "    np.median(all_moments_train[:, 1]),  # nc\n",
    "    np.median(all_moments_train[:, 2]),  # qr\n",
    "    np.median(all_moments_train[:, 3])   # nr\n",
    "])\n",
    "\n",
    "print(f\"\\nAsinh scales (using median):\")\n",
    "print(f\"  qc: {asinh_scales[0]:.2e}\")\n",
    "print(f\"  nc: {asinh_scales[1]:.2e}\")\n",
    "print(f\"  qr: {asinh_scales[2]:.2e}\")\n",
    "print(f\"  nr: {asinh_scales[3]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "akspq2p1u4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moment scaler parameters:\n",
      "  Means: [1.24339712 1.30073491 1.14457747 0.96651209]\n",
      "  Stds: [1.26184572 1.34025571 1.20207609 0.88758972]\n"
     ]
    }
   ],
   "source": [
    "# Apply asinh transformation to training data\n",
    "moments_asinh_train = np.arcsinh(all_moments_train / asinh_scales)\n",
    "\n",
    "# Fit StandardScaler on asinh-transformed moments\n",
    "moment_scaler = StandardScaler()\n",
    "moment_scaler.fit(moments_asinh_train)\n",
    "\n",
    "print(f\"\\nMoment scaler parameters:\")\n",
    "print(f\"  Means: {moment_scaler.mean_}\")\n",
    "print(f\"  Stds: {moment_scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gb2qi4871m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply full transformation (asinh + z-score) to all trajectories\n",
    "def transform_moments(moments, asinh_scales, moment_scaler):\n",
    "    \"\"\"Apply asinh transformation followed by z-score normalization.\"\"\"\n",
    "    moments_asinh = np.arcsinh(moments / asinh_scales)\n",
    "    moments_normalized = moment_scaler.transform(moments_asinh)\n",
    "    return moments_normalized\n",
    "\n",
    "# Apply to training trajectories\n",
    "for traj in train_trajectories:\n",
    "    traj['moments_scaled'] = transform_moments(traj['moments'], asinh_scales, moment_scaler)\n",
    "\n",
    "# Apply to validation trajectories\n",
    "for traj in val_trajectories:\n",
    "    traj['moments_scaled'] = transform_moments(traj['moments'], asinh_scales, moment_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4a31a",
   "metadata": {},
   "source": [
    "We now incorporate Transformed moments to all trajectories as 'moments_scaled' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uu8ps5kko3p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asinh normalization parameters saved to data/asinh_normalization_stats.pkl\n",
      "Asinh scales: [7.81641163e-05 1.27452755e+07 8.67027511e-16 1.99642239e-04]\n",
      "Scaler means: [1.24339712 1.30073491 1.14457747 0.96651209]\n",
      "Scaler stds: [1.26184572 1.34025571 1.20207609 0.88758972]\n"
     ]
    }
   ],
   "source": [
    "# Save asinh normalization parameters for inference\n",
    "asinh_normalization_stats = {\n",
    "    'asinh_scales': asinh_scales,\n",
    "    'moment_scaler_mean': moment_scaler.mean_,\n",
    "    'moment_scaler_std': moment_scaler.scale_,\n",
    "    'moment_scaler': moment_scaler  # Save the full scaler object for convenience\n",
    "}\n",
    "\n",
    "with open(data_dir / 'asinh_normalization_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(asinh_normalization_stats, f)\n",
    "\n",
    "print(\"Asinh normalization parameters saved to data/asinh_normalization_stats.pkl\")\n",
    "print(f\"Asinh scales: {asinh_scales}\")\n",
    "print(f\"Scaler means: {moment_scaler.mean_}\")\n",
    "print(f\"Scaler stds: {moment_scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34fe7d",
   "metadata": {},
   "source": [
    "Each trajectory now contains:\n",
    "- 'moments': raw moment data\n",
    "- 'moments_scaled': asinh-transformed and z-score normalized moments\n",
    "- 'env_params': raw environmental parameters\n",
    "- 'env_params_scaled': z-score normalized environmental parameters\n",
    "- 'length': number of timesteps\n",
    "- 'ic_idx': initial condition index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "w4nxpz5qpkm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled moment statistics:\n",
      "  qc - Mean: +0.000000, Std: 1.000000, Range: [-0.99, +2.13]\n",
      "  nc - Mean: +0.000000, Std: 1.000000, Range: [-0.97, +2.49]\n",
      "  qr - Mean: -0.000000, Std: 1.000000, Range: [-0.95, +3.09]\n",
      "  nr - Mean: +0.000000, Std: 1.000000, Range: [-1.09, +2.29]\n",
      "No NaN or Inf values detected\n"
     ]
    }
   ],
   "source": [
    "# Collect all scaled moments from training data\n",
    "all_moments_scaled_train = np.vstack([traj['moments_scaled'] for traj in train_trajectories])\n",
    "\n",
    "print(f\"\\nScaled moment statistics:\")\n",
    "for i, name in enumerate(['qc', 'nc', 'qr', 'nr']):\n",
    "    mean = all_moments_scaled_train[:, i].mean()\n",
    "    std = all_moments_scaled_train[:, i].std()\n",
    "    min_val = all_moments_scaled_train[:, i].min()\n",
    "    max_val = all_moments_scaled_train[:, i].max()\n",
    "    print(f\"  {name} - Mean: {mean:+.6f}, Std: {std:.6f}, Range: [{min_val:+.2f}, {max_val:+.2f}]\")\n",
    "\n",
    "# Check for NaN or Inf values\n",
    "if np.any(np.isnan(all_moments_scaled_train)) or np.any(np.isinf(all_moments_scaled_train)):\n",
    "    print(\"WARNING: NaN or Inf values detected in scaled moments!\")\n",
    "else:\n",
    "    print(\"No NaN or Inf values detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4919aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:36:19.728842Z",
     "iopub.status.busy": "2025-10-31T19:36:19.728488Z",
     "iopub.status.idle": "2025-10-31T19:36:19.759324Z",
     "shell.execute_reply": "2025-10-31T19:36:19.758859Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_dir / \"train_trajectories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_trajectories, f)\n",
    "\n",
    "with open(data_dir / \"val_trajectories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_trajectories, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
